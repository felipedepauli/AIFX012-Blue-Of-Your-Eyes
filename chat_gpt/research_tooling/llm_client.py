import os
import sys
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

def init_llm(model_name: str = "gpt-4o"):
    # Load .env from project root
    # assuming we are running from repo root
    load_dotenv()
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("ERRO: OPENAI_API_KEY nÃ£o encontrada no .env")
        sys.exit(1)
        
    llm = ChatOpenAI(model=model_name, temperature=0.7)
    return llm

def query_llm(prompt_text: str, model: str = "gpt-4o") -> str:
    """
    Sends a prompt to the LLM and returns the string response.
    """
    try:
        llm = init_llm(model)
        
        # Simple invocation
        # We treat the whole prompt_text as a user message or system+user if properly formatted?
        # The prompt_text generated by prompts.py includes "Objetivo: ... Instructions: ..."
        # So we can pass it as a user message or system message.
        # Given it's a full prompt, let's pass it as a generic message.
        
        chain = llm | StrOutputParser()
        response = chain.invoke(prompt_text)
        return response
    except Exception as e:
        print(f"Erro ao chamar LLM: {e}")
        return ""
